---
title: "Data import"
output: html_notebook
---

## Set up 

```{r setup, include=FALSE}
MyDeleteItems<-ls()
rm(list=MyDeleteItems)
library(tidyverse)
library(readxl)
library(reticulate)
library(lubridate)
use_condaenv(condaenv = 'eikon',conda = "/opt/anaconda3/bin/conda", required=TRUE)
py_run_string('import eikon')
load('dat.RData')

```

```{python}
import eikon as ek  # the Eikon Python wrapper package
import numpy as np  # NumPy
import pandas as pd  # pandas
import cufflinks as cf  # Cufflinks
import configparser as cp
import pandas as pd
import datetime as dt
import dateutil.relativedelta
import eikon as ek
```

## Connect to the Eikon API

This requires a local version of the API proxy running.
[Eikon API information can be found here](https://developers.refinitiv.com/eikon-apis/eikon-data-api)

* For students use: 
* Username: qmseikon3@qub.ac.uk
* password: tY69e66x6WXX

* For staff use:
* Username: qmseikon2@qub.ac.uk         
* Password: QmsStaffMember185

* Admininstrator:
* Username: b.quinn@qub.ac.uk         
* Password: N1pp3r1973

1. turn on API proxy
```{bash}
open -F -a 'Eikon API Proxy'
```


## Download end of day price data for the first time

```{python}
ek.set_app_id("7d75f6e45b9143ae80d3456648399a429e9391f0")
```


```{python, first}
import time
from datetime import datetime
rics=r.RICs_full
rics
start='2016-05-02'
fn = 'eikon_eod_data.csv'
first = True
for ric in rics:
    print(ric)
    try:
      d = ek.get_timeseries(ric,  # the RIC
                             fields='CLOSE',  # the required fields
                             start_date=start,  # start date
                            calendar="tradingdays")
    except:
      pass
    else:
      if first:
        data = d
        data.columns = [ric]
        first = False
      else:
        data[ric] = d
      time.sleep(2)
data.to_csv(fn)
```
```{python, second}
import time
from datetime import datetime
rics=r.RICs_full
rics
start="2017-08-12"
fn = 'eikon_eod_data1.csv'
first = True
for ric in rics:
    print(ric)
    try:
      d = ek.get_timeseries(ric,  # the RIC
                             fields='CLOSE',  # the required fields
                             start_date=start,calendar='tradingdays')  # start date
    except:
      pass
    else:
      if first:
        data1 = d
        data1.columns = [ric]
        first = False
      else:
        data1[ric] = d
      time.sleep(2)
data.to_csv(fn)
```
```{python}
final=data.append(data1)
```

## Download sector price return indices

The .FT codes only go back a year. But the real time indices (.FTUB) seem to go for longer
|Sector_IMF|Eikon Code|Name|
|:--:|:---:|:---:|
|Health Care|.FTUB2010|FTSE 350 Health Care Price Return GBP Real Time|
|Financials|.FTUB3020|FTSE 350 Financial Services Price Return GBP Real Time|
|Materials|.FTUB5010|FTSE 350 Constructions and Materials Price Return GBP Real Time|
|Basic Materials|.FTUB5510|FTSE 350 Basic Resources Price Return GBP Real Time|
|Information Technology|.FTUB1010|FTSE 350 Technology Price Return GBP Real Time|
| |.FTUB5510|FTSE 350 Basic Resources Price Return GBP Real Time|
|Real Estate|.FTUB3510|FTSE 350 Real Estate Price Return GBP Real Time

```{python}
msci=pd.read_csv("MSCI.csv") # Using MSCI index navigator online
sectors=msci.RIC.values.tolist()[0:11] + [".FTLC"]
sectors
```

```{python}
start='2016-05-02'
fn = 'eikon_sector_returns.csv'
import time
first = True
for ric in sectors:
    print(ric)
    d = ek.get_timeseries(ric,  # the RIC
                             fields='CLOSE',  # the required fields
                             start_date=start,calendar='tradingdays')  # start date
                      
    if first:
        indices = d
        indices.columns = [ric]
        first = False
    else:
        indices[ric] = d
    time.sleep(2)
data.to_csv(fn)

```

```{r prices and sectors}
py$final ->prices
py$data_grid ->eikon_holdings_info
py$indices->indices
ind_nams=names(indices)
ind_code_to_names<-tibble(code=ind_nams,Sector=c('Consumer Discretionary',"Consumer Staples","Energy","Financials","Health Care","Industrials","Information Technology","Materials","Utilities","Communication Services","Real Estate","FTSE 350"))
names(indices)<-ind_code_to_names$Sector
indices %>% 
  mutate(Date=parse_date_time(Date,order="ymd")) %>%
  pivot_longer(-Date,names_to="Index",values_to="Price") %>%
  arrange(Index,Date) %>%
  group_by(Index) %>%
  mutate(pr_rtn_ind=log(Price/lag(Price))) %>%
  ungroup() -> indices_df
save.image(file="dat.RData")
```

## Portofolio analytics

```{r portfolio returns}
consolidated %>% 
  distinct(ISIN,.keep_all = T) %>% 
  select(ISIN,Sector,Quantity,ShortName) %>%
  left_join(eikon_holdings_info, by="ISIN")->merged_holdings_info

prices %>% 
  add_rownames(var = "Date") %>%
  pivot_longer(-Date, names_to="RIC",values_to="Price") %>%
  drop_na(Price) %>%
  left_join(merged_holdings_info %>%
  select(Instrument,ShortName,Quantity,Sector,`TRBC Economic Sector Name`) %>% rename(RIC=Instrument), by="RIC") %>%
  mutate(val_gbp=Price*Quantity/100,
         Date=lubridate::parse_date_time(Date,orders = "ymd")) %>%
  drop_na(val_gbp) %>%
  arrange(RIC, Date) %>%
  group_by(RIC) %>%
  mutate(pr_rtn=(Price-lag(Price))/lag(Price))->prices_df
```


```{r graph_cum_returns}
start_date="2016-12-01"
prices_df %>%
  filter(Date>lubridate::parse_date_time(start_date,order="ymd")) %>%
  mutate(cum_rtn=cumsum(coalesce(pr_rtn, 0)) + pr_rtn*0) %>%
  ggplot(aes(y=cum_rtn,x=Date)) +
  geom_line() +
  facet_wrap(~ShortName)
```

```{r app_data}
# Full portfolio
prices_df %>%
  filter(Sector!="Cash") %>%
  group_by(Date) %>%
  mutate(wts=val_gbp/sum(val_gbp,na.rm = T)) %>%
  summarise(smf_weighted_return=sum(wts*pr_rtn)) %>%
  left_join(indices_df %>% 
              filter(Index=="FTSE 350") %>% 
              select(Date,pr_rtn_ind),by="Date") %>%
  drop_na() %>%
  rename(pr_rtn=smf_weighted_return) %>%
  mutate(Sector="Full")->smf_ftse

# Holding returns
prices_df %>%
  ungroup() %>%
  left_join(
    indices_df %>% rename(Sector=Index) %>% filter(Sector!='FTSE 350') %>% select(-Price),
    by=c("Date",'Sector')
  ) %>% select(Date,Sector,pr_rtn,pr_rtn_ind) %>% 
  bind_rows(smf_ftse) %>% 
  group_split(Sector,.keep = T) ->returns
returns[[13]]
names(returns)<-sort(c(unique(prices_df$Sector),"Full"))
saveRDS(returns,"returns.rds")

```


## Point in time FTSE350 constituent list
The below code builds a point in time FTSE350 constituents list, using leavers and joiners at the end of each month.

```{python eval=FALSE, include=FALSE}
ic, err = ek.get_data('.FTLC', ['TR.IndexConstituentRIC'])
lj, err = ek.get_data('.FTLC', 
                      ['TR.IndexJLConstituentChangeDate',
                       'TR.IndexJLConstituentRIC.change',
                       'TR.IndexJLConstituentRIC'],
                     {'SDate':'0D', 'EDate':'-55M', 'IC':'B'})
lj['Date']=pd.to_datetime(lj['Date']).dt.date
lj.sort_values(['Date','Change'],ascending=False,inplace=True)
dates = [dt.date(2016,4,30)]
i = 0
while (dates[0] + dateutil.relativedelta.relativedelta(months=+i+1)) < dt.date.today():
    dates.append(dates[0] + dateutil.relativedelta.relativedelta(months=+i+1))
    i = i + 1
dates.append(dt.date.today())
df = pd.DataFrame(index=dates, columns=['Index Constituents'])
ic_list = ic['Constituent RIC'].tolist()
for i in range(len(dates)):
    print(str(dates[len(dates)-i-1]))
    df.at[dates[len(dates)-i-1],'Index Constituents'] = ic_list[:]
    for j in lj.index:
        if lj['Date'].loc[j] <= dates[len(dates)-i-1]:
            if lj['Date'].loc[j] > dates[len(dates)-i-2]:
                if lj['Change'].loc[j] == 'Joiner':
                    print('Removing ' + lj['Constituent RIC'].loc[j])
                    ic_list.remove(lj['Constituent RIC'].loc[j])
                elif lj['Change'].loc[j] == 'Leaver':
                    print('Adding ' + lj['Constituent RIC'].loc[j])
                    ic_list.append(lj['Constituent RIC'].loc[j])
            else:
                break
df.to_csv("~/Dropbox/SMF/OversightCommitte/FTCL_constituents.csv")
```




